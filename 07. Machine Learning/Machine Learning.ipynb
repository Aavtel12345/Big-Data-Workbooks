{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ML1",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "# Machine Learning\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Motivation and Background](#Motivation-and-Background)\n",
    "- [Data basics](#Data-Basics)\n",
    "  - [Exercise 1](#Exercise-1)\n",
    "- [Cleaning and subsetting data](#Understanding-the-Data)\n",
    "  - [Exercise 2](#Exercise-2)\n",
    "- [Model Selection and Assessment](#Model-Selection-and-Assessment)\n",
    "  - [Exercise 3](#Exercise-3)\n",
    "- [References](#References)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ML2",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "# Motivation and Background\n",
    "\n",
    "Research in science policy often involves making use of publically available datasets. As such, most of the data we work with is not perfect. Today, we will be diving into the National Institutes for Health (NIH) grant data. NIH provides a myriad of information about its grants which you can access here: http://projectreporter.nih.gov/reporter.cfm. Unfortunately, since the information draws on disparate sources like eRA databases, Medline, PubMed Central, the NIH Intramural Database, and iEdison, it is often not complete. \n",
    "\n",
    "In this workbook, we will examine one application of machine learning that deals with predicting missing information. Often in sciece policy research, we are interested in knowing what areas of science are being funded. We often employ diverse techniques to determine that, including text analytics like topic modeling (more on that in the next workbook). However, we can also just simply look at the department of the PI. NIH provides just such a variable in its data. Unfortunately, this variable is often missing. In this workbook, we will walk through the process of imputing values for such a missing categorical variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "ML3",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "## Data basics\n",
    "\n",
    "The data we need for this exercise is available from the 'umetricsgrants' database on the class server. You learnt last week how to obtain data from a MySQL database using python. At this point, you should refer to the data schema provided to you in the first class. In 'nih_project', there is a variable 'ORG_DEPT' that details the department of the PI of the grant. This will be our outcome variable of interest, and for the purpose of this exercise, you are allowed to use any variables in the tables that have the 'nih' prefix as predictors. However, to get us started, I have created an extract of the data and placed it in the 'homework' in a table called 'MachineLearning'. In this workbook, we will be using the pandas package to read in and manipulate data. Pandas provides an alternative to reading data from MySQL that is more efficient since pandas can directly be used for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np\n",
    "from IPython.core.display import Image\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a sql engine. Replace your MySQL credentials in the statement below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdb = create_engine(\"mysql://<username>:<password>@localhost:3306/homework?charset=utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Below we get the data stored in MachineLearning table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_sql('Select * from homework.MachineLearning', pdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets look at what the data looks like. The simple command data.head() gives us a sneak peek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our data is represented by a DataFrame. You can think of data frames as a giant spreadsheet which you can program. It's a collection of series (or columns) with a common set of commands that make managing data in Python super easy. For example, if you want to look at the last five rows of ORG_DEPT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[\"ORG_DEPT\"].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the data is stored internally, we can use the data.dtypes command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lets look at the variables one by one:\n",
    "\n",
    "* APPLICATION_ID: Unique identifier for each grant\n",
    "* CFDA_CODE: CFDA contains detailed program descriptions for 2,292 Federal assistance [programs](#https://www.cfda.gov/)\n",
    "* YEAR: Year in which grant was awarded\n",
    "* ACTIVITY:A 3-character code identifying the grant, contract, or intramural activity through which a project is supported. Here is a list of activity [codes](#http://grants.nih.gov/grants/funding/ac_search_results.htm)\n",
    "* ADMINISTERING_IC: Administering Institute or Center - A two-character code to designate the agency, NIH Institute, or Center administering the grant. See [definitions](#http://grants.nih.gov/grants/glossary.htm#I14).\n",
    "* ARRA_FUNDED: “Y” indicates a project supported by funds appropriated through the American Recovery and Reinvestment Act of 2009.\n",
    "* ORG_NAME:  The name of the educational institution, research organization, business, or government agency receiving funding for the grant, contract, cooperative agreement, or intramural project.  \n",
    "* ORG_DEPT:  The departmental affiliation of the contact principal investigator for a project, using a standardized categorization of departments.  Names are available only for medical school departments.\n",
    "* STUDY_SECTION:  A designator of the legislatively-mandated panel of subject matter experts that reviewed the research grant application for scientific and technical merit.\n",
    "* TOTAL_COST: Total project funding from all NIH Institute and Centers for a given fiscal year.\n",
    "* TOPIC_ID: Using text analysis techniques, a topic_id was assigned to each grant. This topic_id is a key for that topic. You can see what the topic contains by looking in the `topiclda_text` table in `umetricsgrants` database.\n",
    "* ED_INST_TYPE:  Generic name for the grouping of components across an institution who has applied for or receives NIH funding.\n",
    "\n",
    "You are free to use more predictor variables than the ones listed above from the`nih_project` table in `umetricsgrants` database. A complete description of all variables is available [here](#http://exporter.nih.gov/about.aspx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Pandas provide some great functions for descriptive statistics. Complete the function below to get summary statistics for variables in the dataset, as well as print out a table of top 10 department names. Look into the describe, head, and value_counts functions to achieve that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "ML_Ex1",
     "locked": false,
     "points": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def printDescrriptiveStats(data):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : A pandas DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    summary : A Pandas dataframe containing count, mean, standard deviation, \n",
    "              minimum, maximum, and the 25th, 50th and 75th percentile\n",
    "    topDepts : A pandas.core.series.Series containing the top 10 departments\n",
    "               and their frequencies\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    \n",
    "    return data.describe(), data[\"ORG_DEPT\"].value_counts().head(10) \n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets see what our data looks like\n",
    "printDescrriptiveStats(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and subsetting Data\n",
    "\n",
    "Looking at the data, it is clear that there are a lot of missing values in ORG_DEPT. Let us do some basic cleaning of the data. For that, we first need to figure out what variables have missing values. Here is a handy function to do that:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_null_freq(df):\n",
    "    \"\"\"\n",
    "    for a given DataFrame, calculates how many values for \n",
    "    each variable is null and prints the resulting table to stdout\n",
    "    \"\"\"\n",
    "    df_lng = pd.melt(df)\n",
    "    null_variables = df_lng.value.isnull()\n",
    "    return pd.crosstab(df_lng.variable, null_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets see what our NULL values look like\n",
    "print_null_freq(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have a better understanding of the issues our dataset has, lets go ahead and write some code to deal with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "\n",
    "Complete the function below to take in a dataframe, a column name in the dataframe, a value, and return a cleaned dataframe that is subsetted to remove the rows containing the specified value in the specified column. Your function should be equipped to deal with Null values specified in the value parameter. Look into the notnull() function in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "ML_Ex2",
     "locked": false,
     "points": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def cleanData(data, column, value):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : A pandas DataFrame\n",
    "    column : Name of the column on the dataframe\n",
    "    value : The value to be removed from the dataframe in the specified column.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    cleanData : A Pandas dataframe containing only rows that did not have the \n",
    "                specified value in the specified column\n",
    "    \"\"\"\n",
    "    if(column not in list(data.columns.values)):\n",
    "        print(\"ERROR : Column you specified not present in the dataframe\")\n",
    "        return\n",
    "    \n",
    "    ### BEGIN SOLUTION\n",
    "    if value.upper() == \"NULL\":\n",
    "        dataClean = data[pd.notnull(data[column])]\n",
    "    else:\n",
    "        dataClean = data[data[column] != value]\n",
    "    return dataClean\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, if the department name is missing, we cannot use that data to train our classifier. Fortunately, we have our cleanData function that will take care of the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataC = cleanData(data, \"ORG_DEPT\", \"null\")\n",
    "print_null_freq(dataC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That did clean most of the NULL values for us. There might be other values in ORG_DEPT we might want to clean, so lets print a frequency table for ORG_DEPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataC[\"ORG_DEPT\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of NONE, MISCELLANEOUS and No CODE ASSIGNED are also useless for us. Similarly, we should standardize the department names by converting everything to upper case. In fact, let us go one step further and convert all categorical variables to upper case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Getting rid of NONE and NO CODE ASSIGNED.\n",
    "dataC = cleanData(dataC, \"ORG_DEPT\", \"NONE\")\n",
    "dataC = cleanData(dataC, \"ORG_DEPT\", \"NO CODE ASSIGNED\")\n",
    "dataC = cleanData(dataC, \"ORG_DEPT\", \"MISCELLANEOUS\")\n",
    "\n",
    "# Converting to upper case\n",
    "for col in list(data.columns.values):\n",
    "    if dataC[col].dtype == np.object_:\n",
    "        dataC[col] = dataC[col].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_null_freq(dataC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since YEAR, STUDY_SECTION and CFDA_CODE are categorical variables, there is no easy way to impute them. So lets do listwise deletion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataC = cleanData(dataC, \"CFDA_CODE\", \"NULL\")\n",
    "dataC = cleanData(dataC, \"STUDY_SECTION\", \"NULL\")\n",
    "dataC = cleanData(dataC, \"YEAR\", \"NULL\")\n",
    "\n",
    "# Lets see if we have any more null frequencies to deal with\n",
    "print_null_freq(dataC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have 48 missing values for TOTAL_COST. We can get rid of them, but we can also impute them by taking the mean. In this case, it makes more sense to delete these rows since we have a total of 367,691 records and deleting 48 rows will not make much difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataC = cleanData(dataC, \"TOTAL_COST\", \"NULL\")\n",
    "print_null_freq(dataC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take another look at our ORG_DEPT variable, and see if we can combine some departments that are very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataC[\"ORG_DEPT\"].value_counts()\n",
    "len(dataC.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It certainly looks like a lot of these departments can be combined into broader areas. This would reduce the number of categories to predict and our model will make better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataC.loc[dataC.ORG_DEPT == \"PATHOLOGY\",['ORG_DEPT']] = \"MEDICINE\"\n",
    "dataC.loc[dataC.ORG_DEPT == \"INTERNAL MEDICINE/MEDICINE\",['ORG_DEPT']] = \"MEDICINE\"\n",
    "dataC.loc[dataC.ORG_DEPT == \"FAMILY MEDICINE\",['ORG_DEPT']] = \"MEDICINE\"\n",
    "dataC.loc[dataC.ORG_DEPT == \"NEUROSURGERY\",['ORG_DEPT']] = \"MEDICINE\"\n",
    "dataC.loc[dataC.ORG_DEPT == \"PLASTIC SURGERY\",['ORG_DEPT']] = \"MEDICINE\"\n",
    "dataC.loc[dataC.ORG_DEPT == \"OBSTETRICS &GYNECOLOGY\",['ORG_DEPT']] = \"MEDICINE\"\n",
    "dataC.loc[dataC.ORG_DEPT == \"PHARMACOLOGY\",['ORG_DEPT']] = \"MEDICINE\"\n",
    "dataC.loc[dataC.ORG_DEPT == \"ANESTHESIOLOGY\",['ORG_DEPT']] = \"MEDICINE\"\n",
    "dataC.loc[dataC.ORG_DEPT == \"RADIATION-DIAGNOSTIC/ONCOLOGY\",['ORG_DEPT']] = \"MEDICINE\"\n",
    "dataC.loc[dataC.ORG_DEPT == \"DERMATOLOGY\",['ORG_DEPT']] = \"MEDICINE\"\n",
    "dataC.loc[dataC.ORG_DEPT == \"SURGERY\",['ORG_DEPT']] = \"MEDICINE\"\n",
    "dataC.loc[dataC.ORG_DEPT == \"EMERGENCY MEDICINE\",['ORG_DEPT']] = \"MEDICINE\"\n",
    "dataC.loc[dataC.ORG_DEPT == \"DENTISTRY\",['ORG_DEPT']] = \"MEDICINE\"\n",
    "dataC.loc[dataC.ORG_DEPT == \"ORTHOPEDICS\",['ORG_DEPT']] = \"MEDICINE\"\n",
    "dataC.loc[dataC.ORG_DEPT == \"NEUROLOGY\",['ORG_DEPT']] = \"MEDICINE\"\n",
    "dataC.loc[dataC.ORG_DEPT == \"PEDIATRICS\",['ORG_DEPT']] = \"MEDICINE\"\n",
    "dataC.loc[dataC.ORG_DEPT == \"PHYSIOLOGY\",['ORG_DEPT']] = \"BIOLOGY\"\n",
    "dataC.loc[dataC.ORG_DEPT == \"OTOLARYNGOLOGY\",['ORG_DEPT']] = \"MEDICINE\"\n",
    "dataC.loc[dataC.ORG_DEPT == \"UROLOGY\",['ORG_DEPT']] = \"MEDICINE\"\n",
    "\n",
    "dataC.loc[dataC.ORG_DEPT == \"ANATOMY/CELL BIOLOGY\",['ORG_DEPT']] = \"BIOLOGY\"\n",
    "dataC.loc[dataC.ORG_DEPT == \"OPHTHALMOLOGY\",['ORG_DEPT']] = \"MEDICINE\"\n",
    "dataC.loc[dataC.ORG_DEPT == \"ZOOLOGY\",['ORG_DEPT']] = \"BIOLOGY\"\n",
    "dataC.loc[dataC.ORG_DEPT == \"BIOMEDICAL ENGINEERING\",['ORG_DEPT']] = \"ENGINEERING (ALL TYPES)\"\n",
    "dataC.loc[dataC.ORG_DEPT == \"ZOOLOGY\",['ORG_DEPT']] = \"BIOLOGY\"\n",
    "dataC.loc[dataC.ORG_DEPT == \"PHYSICAL MEDICINE &REHAB\",['ORG_DEPT']] = \"MEDICINE\"\n",
    "dataC.loc[dataC.ORG_DEPT == \"PUBLIC HEALTH &PREV MEDICINE\",['ORG_DEPT']] = \"OTHER HEALTH PROFESSIONS\"\n",
    "dataC.loc[dataC.ORG_DEPT == \"NUTRITION\",['ORG_DEPT']] = \"OTHER CLINICAL SCIENCES\"\n",
    "\n",
    "# We will also get rid of \"ADMINISTRATION\n",
    "dataC = cleanData(dataC, \"ORG_DEPT\", \"ADMINISTRATION\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Assessment\n",
    "\n",
    "Now that we have a clean dataset, we can move on to the fun parts!! The python machine learning libraries do not accept categorical variables, so we need to convert all such variables to dummies first. However, pandas makes it super easy! \n",
    "\n",
    "But before we do that, lets split our data variables into predictors (features) and predicted, or dependent and independent variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Lets go ahead and split into predictors and predicted\n",
    "features = [x for x in list(dataC.columns.values) if x not in  [\"ORG_DEPT\"]]\n",
    "data_x = dataC[features]\n",
    "data_y = dataC[\"ORG_DEPT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can easily converty all categorical variables in `data_x` into dummy/binary variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python's sckikit algorithms dont work on categorical variables. Fortunately, Pandas provides an easy way out!\n",
    "\n",
    "data_x = pd.get_dummies(data_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're building a model, we're going to need a way to know whether or not it's working. Convincing other is oftentimes the most challenging parts of an analysis. Making repeatable, well documented work with clear success metrics makes all the difference.\n",
    "\n",
    "For our classifier, we're going to use the following build methodology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(url=\"https://s3.amazonaws.com/demo-datasets/traintest.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have limited number of features, I wont be going into any feature engineering examples. However, here is a good [tutorial](#http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)\n",
    "\n",
    "Let us now split our dataset into test and training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_x, data_y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's `scikit-learn` is a very well known machine library. It is also well documented and maintained. You can learn all about it [here](#http://scikit-learn.org/stable/). We will be using different classifiers from this library for our predictions in this workbook. \n",
    "\n",
    "We will start with the simplest `Logistic Regression` model and see how well we do. You can use any number of metrics to judge your models, but we will be using the accuracy score as our measure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets fit the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we print the model, we see different parameters we can adjust to make the model. Using cross-validation is a good way to fine-tune the parameters. A good tutorial on cross-validation can be found [here](#http://scikit-learn.org/stable/modules/cross_validation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the model we just fit to make predictions on our test dataset, and see what our accuracy score is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expected = y_test\n",
    "predicted = model.predict(X_test)\n",
    "accuracy_score(expected, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an accuracy score of 46%. This is not a great score, however, it is much better than random guessing, which would have had a chance of 1/18 of succeeding. The other way to guess would be to take the mode, which in this case is MEDICINE with a frequency of 168031, which would give us an accuracy score of  45%. So logistic regression does better than both. Let's see if the other classifiers can do any better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Complete the function below to train different classifiers from the scikit library. Return the accuracy score. Your goal is to come up with a classifier that gives at least 75% accuracy on the test dataset.Feel free to play around with different parameters of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "ML_Ex3",
     "locked": false,
     "points": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def classifier(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : A pandas DataFrame of features used for training the classifier\n",
    "    y_train : A pandas dataframe of y values used for training the classifier\n",
    "    X_test, y_test : Use these to test the accuracy of your classifier\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    accuracy score : a float giving the percent of accurate predictions you made\n",
    "    \"\"\"\n",
    "   \n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    expected = y_test\n",
    "    predicted = model.predict(X_test)\n",
    "    return accuracy_score(expected, predicted)*100\n",
    "    ### END SOLUTION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* [Scikit-Learn Documentation](#http://scikit-learn.org/stable/)\n",
    "* [NIH Reporter Documentation](#http://exporter.nih.gov/about.aspx)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
