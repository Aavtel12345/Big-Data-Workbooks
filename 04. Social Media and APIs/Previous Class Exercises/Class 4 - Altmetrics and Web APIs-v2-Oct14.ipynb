{
 "metadata": {
  "name": "",
  "signature": "sha256:d0ecc5bc3c4bc642879ad6956f997d0447b996e9c6410022e0dc83b6cfb3b67d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use and manipulation of Web APIs to track research impact\n",
      "=========================================================\n",
      "\n",
      "Many online data sources provide a web-based application programming interface (API) that allows interested parties to pull data. Much of the conversation about 'mash-ups' revolves around combining and visualising data from such APIs. In the context of research outputs there are APIs that provide access to information about published papers, including bibliographic inforamtion and usage statistics. In this session we will use two APIs provided by PLOS and one from the social media provider Twitter to examine the conversations occuring online about papers published from CalTech.\n",
      "\n",
      "Using an API\n",
      "------------\n",
      "\n",
      "APIs come in many forms. A common form of modern API is a so-called 'RESTful API' which is generally used to mean those that are accessed through simple web protocols. A client queries a specific URL and receives data in return. A URL might look like:\n",
      "\n",
      "  http://oag.cottagelabs.com/lookup/10.1371/journal.pone.0043623.json\n",
      "    \n",
      "This URL contains two parts, a root URL (http://oag.cottagelabs.com/lookup/) and a data 'query' in this case made up of a DOI and a data type specifier. Click on the link above to see the data that is returned. This particular API provides information on the licenses associated with the paper that is found by following a DOI, in this case the paper at http://dx.doi.org/10.1371/journal.pone.0043623. The '.json' part specifies that the result should be provided in JSON.\n",
      "\n",
      "Testing an API using Hurl.it\n",
      "----------------------------\n",
      "\n",
      "A useful service called http://hurl.it can be used to test and play with web APIs. Follow the link and then paste in the URL above to see the result. You can also change the query to be a POST request or remove the .json and add an `Accept` header of `application/json`. Hurl.it can be very useful for trouble shooting an API or just to understand how it works and what it returns.\n",
      "\n",
      "API keys\n",
      "--------\n",
      "\n",
      "Many production APIs require you to obtain an API 'key' that identifies you as a registered user to the API administrator. This can be used to control access to data (although it's not very secure) but is more often used to regulate access to a reasonable number of requests and to monitor the usage so any issues can be raised with clients. Generally registration is an automated process. In this class we will use two PLOS APIs that require an API key.\n",
      "\n",
      "You can register for a PLOS account at http://register.plos.org. Once you have created an account you can then obtain your API key by navigating to http://alm.plos.org, logging in, and then navigating to 'your account' from the menu at the top right. Your API key should be near the top of the page.\n",
      "\n",
      "Testing the PLOS API\n",
      "--------------------\n",
      "\n",
      "With your API key, return http://hurl.it and run a query to the following URL substituting your API key where shown:\n",
      "\n",
      "[http://api.plos.org/search?q=\"California+Institute+of+Technology\"&api_key=YOUR_API_KEY&wt=json](http://api.plos.org/search?q=\"California%20Institute%20of%20Technology\"&api_key=YOUR_API_KEY&wt=json)\n",
      "\n",
      "This is a generic search query for \"California Institute of Technology\" which should return a result saying that there are 349 articles that match this query. '+' is a way of encoding a space in a URL. More complex queries are of course possible. A good way of constructing the queries for the PLOS API is to use the [Advanced Search](http://www.plosone.org/search/advanced) functionality to construct and explore the search space and then use this via the API. This [more focussed](http://www.plosone.org/search/advanced?pageSize=15&sort=&queryField=author_affiliate&queryTerm=&unformattedQuery=author_affiliate%3A%22California+Institute+of+Technology%22&journalOpt=all&subjectCatOpt=all&filterArticleTypesOpt=all) search reduces the results to 185 positive hits when we focus on author affiliations rather than free text search.\n",
      "\n",
      "Work with the search, focussing on your chosen PI, to identify whether they have published with PLOS. Consult with colleagues to see which of the set of PIs you are working with have chosen to publish with PLOS."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Part II - Working with APIs via code\n",
      "====================================\n",
      "\n",
      "The second part of the class will invove working with more than one API to obtain information on sets of publications. We will obtain some basic data on social media activity around specific papers and do some visualisations and compare the results with those from a set of comparison data. We will also explore some basics of graphing and data manipulation in Python.\n",
      "\n",
      "When working with APIs in Python it is common practice to use wrappers that expose the API in 'Python-like' ways. Generally these wrappers provide simple methods to 'hit' the API and obtain information in easily workable Python-style objects. In this case we are importing three wrappers, one for searching PLOS articles, one for obtaining usage information on PLOS articles and one for interacting with the twitter API."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import twitter\n",
      "import pyalm.pyalm as pyalm\n",
      "import pyalm.config as config\n",
      "import pyalm.utilities.plossearch as search\n",
      "import pprint\n",
      "\n",
      "#Set your API Key\n",
      "config.APIS['plos']['key'] = 'your_key_here'\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Gathering Relevant DOIs\n",
      "------------------------\n",
      "\n",
      "Firstly we want to obtain some DOIs to work with from the PLOS Search API. I am using a wrapper for the PLOS search API that interacts with the Solr index exposed at http://api.plos.org/search. The API wrapper returns a JSON object containing responses. Lets take a look at the first one of those. In this case we're only getting a DOI back. This is the default return information, but we can also ask for much more information if we want to. We can also run quite sophisticated searches."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Initiate and populate a query object\n",
      "query = search.Request('author_affiliate:\"California Institute of Technology\"')\n",
      "\n",
      "# Initiate the actual API call and get some results\n",
      "response = query.get()\n",
      "response"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "For now we just want to get the relevant list of DOIs out of the returned object. This can be done by looping over the set of records as shown below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dois = []\n",
      "for record in response['response']['docs']:\n",
      "    dois.append(record['doi'][0])\n",
      "    \n",
      "len(dois)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A better, neater way to achieve the same thing is to use a list comprehension. For efficient code a list comprehension almost always beats a for loop. There will be several cases further down this introduction where it is useful to create lists from more complex data objects so I've illustrated this below. There are still more efficient ways to generate an iterator object that would behave similarly but they don't create a list with a `len` function which is quite useful for us here. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[record['doi'][0] for record in response['response']['docs']]\n",
      "len(dois)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dois"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Using the DOIs to get Article Level Metrics Data\n",
      "------------------------------------------------\n",
      "\n",
      "We now have a list of DOIs in a form we can use to query the PLOS Article Level Metrics API. Again we're going to use a wrapper to do this. The wrapper has a similar pattern of use. The API docs say to fetch only up to 50 sets of ALMs at a time so we will pull them in batches. When doing this it is always best to be polite. This is only a small set, but if there were thousands of articles we were interested in we would need to consider putting some delays in here."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alms = []\n",
      "for start, stop in [[0,50],[50,100],[100,150],[150,193]]:\n",
      "    response = pyalm.get_alm(dois[start:stop])\n",
      "    alms.extend(response)\n",
      "alms[0:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each ArticleALM object has a range of internal objects that contain information on each of the metrics that PLOS collects. These are derived from various data providers and are called 'sources'. Each can be accessed by name from a dictionary called 'sources'. The iterkeys() function provides an interator that lets us loop over the set of keys in a dictionary.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "article = alms[0]\n",
      "for source in article.sources.iterkeys():\n",
      "    print source"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Analysis of Social Media Activity Around Articles\n",
      "-------------------------------------------------\n",
      "\n",
      "We're looking at social media and twitter data in particular, so how many tweets or Facebooks posts reference this specific article?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print article.sources['twitter'].metrics.total, 'total tweets to this article'\n",
      "print article.sources['facebook'].metrics.total, 'total facebook references to this article'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is quite a lot of activity as the majority of articles get small amounts of social media activity. What we will do is sort the list of articles by number of tweets so that we can easily pull off the most interesting ones. To use the sort function we will want to define a custom comparison funtion. Check the [python documentation for the 'list' type](https://docs.python.org/2.7/library/stdtypes.html#mutable-sequence-types) for details of list.sort(). It's always good to check that your sort put things in the right order."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def numtweets(article1, article2):\n",
      "    comp = article2.sources['twitter'].metrics.total - article1.sources['twitter'].metrics.total\n",
      "    return comp\n",
      "\n",
      "alms.sort(numtweets)\n",
      "alms[0].sources['twitter'].metrics.total"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So what is the article with the most tweets then? Or even the top five?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for article in alms[0:6]:\n",
      "    print article.title, '--Tweets:', article.sources['twitter'].metrics.total"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Obviously just counting the number of tweets isn't in itself terribly interesting. On a large scale these numbers might be informative but for such a relatively small set we will have a lot of conflicting issues contributing to the numbers. Lets take a quick look at the tweets referencing article number three. First we will need to get the actual `event` data for those tweets. Thus far we've just got summary data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "doi = alms[2].doi\n",
      "detail = pyalm.get_alm([doi], info='event')\n",
      "# even tho there is only one article here, it still returns a list with just one element in it\n",
      "pprint.pprint(detail[0].sources['twitter'].events[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The pyalm wrapper has some convenient functions for cleaning up the event data but for this simple case we can just iterate through and compare the text and user names."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tweets = detail[0].sources['twitter'].events\n",
      "for tweet in tweets:\n",
      "    print \"%s: %s\\n\" % (tweet.get('event').get('user'), tweet.get('event').get('text'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We might want to know whether any of the people tweeting are authors on the article. For this we will need to get the author list. This isn't included in the ALM information but it can be obtained from the search API. In addition the full user name can be obtained from Twitter. Twitter doesn't require this to be a \"real\" name (whatever that means) but for researchers it often is."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "query = search.Request('id:%s' % str(doi))\n",
      "query.add_field('author')\n",
      "response = query.get()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "authors = [author for author in response.get('response').get('docs')[0].get('author')]\n",
      "print authors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "users = [tweet.get('event').get('user_name') for tweet in tweets]\n",
      "for user in users:\n",
      "    for author in authors:\n",
      "        if user.split(' ')[0] in author:\n",
      "            print 'Article Author:%s       Twitter Handle:%s' % (author, user)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So it appears that at least one of our tweeters is an author on the paper. Some things to note here. This was a very rough matching algorithm. This kind of matching will be obviously susceptible to false negatives (where an author doesn't give their 'real' name) and potentially positives depending on how strict you make the checks. If you imagine trying to do a match across the full set of authors in your database this would be very messy. This illustrates the importance of unique identifiers and high quality cross walks between databases."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the same API at a different site\n",
      "--------------------------------------\n",
      "\n",
      "You may find the authors or institutions of interest to you don't show very many results. This may be because our results thus far are limited to articles publishing by PLOS. Crossref, an organisation that works across publishers, providing DOIs, is running a pilot program, called \"DOI Event Tracking\" (DET) using the same application stack but applied to all articles with Crossref DOIs from 2012-2014 although it provides fewer data sources. This pilot service uses the same API, so we can use the same wrapper, we just need to add a few settings."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First we need a new API key for the DET service. The Crossref DOI Event Tracker API requires a Mozilla Persona Account. You can register for a persona account at: https://login.persona.org/ Once you have that navigate to http://det.labs.crossref.org, login using the Persona account and as above, navigate to \u2018your account\u2019 to get your API key. With this in hand we need to set up our wrapper to use the DET service rather than the PLOS one."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set the URL for the DET service\n",
      "config.APIS['det'] = {\n",
      "                      'url' : 'http://det.labs.crossref.org/api/v3/articles',\n",
      "                      'key' : 'your_key_here'\n",
      "                      }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As before we first need to get a list of DOIs. The Crossref metadata search API provides a general way of obtaining DOIs based on a search key."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "query = \"author:'Cameron Neylon'\"\n",
      "q = {'q' : query,\n",
      "     'rows' : 99}\n",
      "r = requests.get('http://search.crossref.org/dois', params = q)\n",
      "crossref_ids = [result.get('doi') for result in r.json() if (int(result.get('year', 1)) > 2012)]\n",
      "print len(crossref_ids), crossref_ids[0:3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We need to clean up the DOIs so they are in the http form to use them in the API wrapper. This ought to be a TODO to clean up the wrapper properly."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "crossref_dois = [doi[18:len(doi)] for doi in crossref_ids]\n",
      "crossref_dois[0:3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "crossref_alms = pyalm.get_alm(crossref_dois, instance='det')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for article in crossref_alms:\n",
      "    print article.title\n",
      "    for source in article.sources.iterkeys():\n",
      "        print source, article.sources[source].metrics.total"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the Twitter API\n",
      "---------------------\n",
      "\n",
      "Lets look at the people tweeting and see who has the most reach on Twitter using the Twitter API. Because of rate limits on the Twitter API this will almost certainly be a canned demo but we will see what we can do. Twitter requires OAuth authentication to get interesting information (like number of followers and things like that) so to get the appropriate keys you will need to register for a twitter account and then for an App. I will be obfuscating my keys here."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import twitter\n",
      "twitter_api = twitter.Api(consumer_key='consumer_key',\n",
      "                      consumer_secret='consumer_secret',\n",
      "                      access_token_key='access_token_key',\n",
      "                      access_token_secret='access_token_secret')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I'm using the python-twitter API wrapper to do some calls to find out who amongst those talking about this paper has the most reach. This would be the logical place to start some more sophisticated social network analysis to look at who is talking about which papers and where those conversations go to. You could also imagine looking at network statistics or data mining of those users tweet streams to understand *who* they are, for instance nurses talking about primary care research.\n",
      "\n",
      "We will need a data structure for this. I am using a dictionary of dictionaries. We have one for each user, which is keyed off the screen name. Each dictionary will then be populated with information about the user, including the relevant tweet texts. Then for each unique user we make a call to the twitter API."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "twits = {}\n",
      "for tweet in tweets:\n",
      "    t = tweet.get('event')\n",
      "    user = twits.get(t.get('user'), {})\n",
      "    user.update({\n",
      "                              'user_name' : t.get('user_name'),\n",
      "                              'screen_name' : t.get('user')\n",
      "                           })\n",
      "    \n",
      "    ttexts = user.get('tweet_text', [])\n",
      "    ttexts.append(t.get('text'))\n",
      "    turls = user.get('tweet_urls', [])\n",
      "    turls.append(tweet.get('event_url'))\n",
      "    \n",
      "    user['tweet_text'] = ttexts\n",
      "    user['tweet_urls'] = turls\n",
      "    twits[t.get('user')] = user"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "for user_name in [t for t in twits.iterkeys()]:\n",
      "    try:\n",
      "        followers = twitter_api.GetFollowerIDs(screen_name=user_name)\n",
      "        print len(followers)\n",
      "    except twitter.TwitterError, e:\n",
      "        print e, 'Sleeping for 15 minutes'\n",
      "        time.sleep(15*60)\n",
      "        followers = twitter_api.GetFollowerIDs(screen_name=user_name)\n",
      "\n",
      "    twits[user_name]['num_followers'] = len([f for f in followers])\n",
      "    twits[user_name]['followers'] = followers"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "   for user_name in [t for t in twits.iterkeys()]:\n",
      "    try:\n",
      "        user = twitter_api.GetUser(screen_name=user_name)\n",
      "    except twitter.TwitterError, e:\n",
      "        print e, 'Sleeping for 15 minutes'\n",
      "        time.sleep(15*60)\n",
      "        user = twitter_api.GetUser(screen_name=user_name)\n",
      "        \n",
      "    twits[user_name]['twitter_user'] = user"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for username in twits.iterkeys():\n",
      "    print '%s:%s' % (username, twits[username]['num_followers'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for user in twits.iterkeys():\n",
      "    twits[user]['follower_in_set'] = []\n",
      "    \n",
      "for user1, user2 in [(x,y) for x in twits.iterkeys() for y in twits.iterkeys()]:\n",
      "    if twits[user2]['twitter_user'].id in twits[user1]['followers']:\n",
      "        f = twits.get(user1).get('follower_in_set', [])\n",
      "        f.append(user2)\n",
      "        twits[user1]['follower_in_set'] = f"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for username in twits.iterkeys():\n",
      "    print '%s:%s' % (username, twits[username].get('follower_in_set'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}