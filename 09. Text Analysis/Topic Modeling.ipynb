{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling Using MALLET <a id='table'></a>\n",
    "For this exercise, we will be using data contained in the \"homework\" database on the Big Data for Social Science Class Server. This notebook will walk you through topic modeling NIH abstracts using [MALLET.](#http://mallet.cs.umass.edu/topics.php)\n",
    "\n",
    "## Table of Contents\n",
    "- [Getting Data](#getting_data)\n",
    "  - [Exercise 1](#exercise_1)\n",
    "- [Generating Topics](#generating_topics)\n",
    "  - [Exercise 2](#exercise_2)\n",
    "- [Inferencing Topics](#inferencing_topics)\n",
    "  - [Exercise 3](#exercise_3)\n",
    "- [Resources](#resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data <a id='getting_data'></a>\n",
    "\n",
    "* Back to the [Table of Contents](#table)\n",
    "\n",
    "We will be using the NIH abstracts stored in the 'TextAnalysis' table in the 'homework' database. This table was created by taking a sample of abstracts from the broader 'umetricsgrants' database.\n",
    "\n",
    "Mallet is a Java based text analysis tool that makes topic modeling very easy. However, mallet is primarily a command line tool and requires a specific format for its data. You can read more about importing data into mallet [here.](http://mallet.cs.umass.edu/import.php)\n",
    "\n",
    "We will create a text file for reach individual abstract. Let us first create a temporary directory in our home folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python can send commands to the terminal using \"subprocess\" so that you never have to leave the iPython notebook. We will use the 'subprocess' module to create an empty 'temp' directory. For your convenience, the terminal() function has already been created. It takes in a list of arguments and returns the output from executing the command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the modules we will use in this workbook\n",
    "from subprocess import Popen, PIPE\n",
    "import os\n",
    "import MySQLdb\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import snowball\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def terminal(args):\n",
    "    pipe = Popen(args, stdout = PIPE, stderr=PIPE)\n",
    "    text, err = pipe.communicate()    \n",
    "    text = text.decode()\n",
    "    err = err.decode()\n",
    "    if len(text) > 2:\n",
    "        return text\n",
    "    elif len(err) > 2:\n",
    "        return err\n",
    "    else:\n",
    "        print(\"No output returned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following call will make a temporary directory in your current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terminal(['mkdir', 'temp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets retrieve the abstracts, their ids, and store each abstract in a file with the id as the filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create MySQL connection\n",
    "user = \"<user>\"\n",
    "password = \"<password>\"\n",
    "database = \"homework\"\n",
    "\n",
    "# invoke the connect() function, passing parameters in variables.\n",
    "db = MySQLdb.connect( user = user, passwd = password, db = database )\n",
    "\n",
    "# output basic database connection info.\n",
    "print( db )\n",
    "\n",
    "cursor = db.cursor( MySQLdb.cursors.DictCursor )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create a function that takes in a filename and text, and creates a new file populated with the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeFile(filename, data):\n",
    "    f = open(filename, \"w\")\n",
    "    f.write(str(data))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrote a function to do some initial cleaning of the abstracts. You will notice that we are removing words that would be ver common in NIH abstracts, because we dont want them to bias the results. We are also removing stopwords (MALLET can also do that), as well as punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanAbstract(text):\n",
    "    st = snowball.EnglishStemmer()\n",
    "    commonWords = ['study', 'project', 'experiment', 'abstract', 'description', 'studies', \\\n",
    "                  'abstracts', 'projects', 'experiments', 'descriptions']\n",
    "    text = re.sub('[\\n\\t\\r\\f]+', '', text).lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    from nltk.corpus import stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    tokens = [t for t in tokens if t not in stop]\n",
    "    exclude = set(string.punctuation)\n",
    "    tokenNew=[]\n",
    "    for s in tokens:\n",
    "        snew = ''.join(ch for ch in s if ch not in exclude)\n",
    "        if snew!=\"\":\n",
    "            tokenNew.append(snew)\n",
    "    tokenNew = [t for t in tokenNew if t not in commonWords]\n",
    "    abstract  = ' '.join(t for t in tokenNew)\n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 <a id='exercise_1'></a>\n",
    "\n",
    "* Back to the [Table of Contents](#table)\n",
    "\n",
    "Retrieve the abstracts one by one from the database and write them to text files in the temp directory. For your convenience, the writeFile() function has already been created. You just need to call it with the full path of the filename and the contents of the abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# First create the query that you need to get the abstracts\n",
    "query = 'SELECT * FROM TextAnalysis where TextAnalysis.ABSTRACT_TEXT is not NULL  LIMIT 1000;'\n",
    "\n",
    "#Execute the query\n",
    "cursor.execute(query)\n",
    "\n",
    "#Fetch the results one by one and write them to a file\n",
    "row = cursor.fetchone()\n",
    "while (row is not None):\n",
    "    ID = row['APPLICATION_ID']\n",
    "    abstract = row['ABSTRACT_TEXT']\n",
    "    abstract = cleanAbstract(abstract)\n",
    "    filename = './temp/' + str(ID) + \".txt\"\n",
    "    writeFile(filename, abstract)\n",
    "    row = cursor.fetchone()\n",
    "    \n",
    "\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Topics <a id='generating_topics'></a>\n",
    "\n",
    "* Back to the [Table of Contents](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a number of .txt files in the temp directoty. Each one of these files is a single abstract, and the set of all these files together is a corpus of data. Our next task is to transform these individual files into a single MALLET format. To achieve this, we will use the import command. The import command can read in an entire directory, turn it into a mallet file, and can also strip out common english stopwords. Our command will look something like this:\n",
    "\n",
    "/bin/mallet/bin/mallet import-dir --input path/to/temp/directory --output data.mallet --keep-sequence --remove-stopwords\n",
    "\n",
    "Lets decompose this command:\n",
    "- /bin/mallet/bin/mallet is the path to the mallet program\n",
    "- import-dir is a mallet command that tells mallet to import an entire directory\n",
    "- --input \"--\" are used in mallet to signify commands, and \"-\" is used to signify spaces. --input tells mallet where the corpus of data is located\n",
    "- /path/to/temp/directory Actual path of corpus of data\n",
    "- --output tells mallet where to store the output\n",
    "- --keep-sequence keep the original texts in the order in which they were listed\n",
    "- --remove-stopwords removes common english stopwords like a, an, the.\n",
    "\n",
    "Now use the subprocess.Popen() command to run the mallet import command. Remeber, terminal() takes in a list of arguments, so decompose the import command into a list of arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "args  = ['/bin/mallet/bin/mallet', 'import-dir', '--input',  './temp', '--output', \\\n",
    "         'data.mallet', '--keep-sequence', '--remove-stopwords']\n",
    "print(terminal(args))\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you go to your working directory, you will find a data.mallet file. This is the input file that mallet will use to generate topics.\n",
    "\n",
    "You can use the train-topic command in mallet to generate your very own topic models. We will execute this command below using only the default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args = ['/bin/mallet/bin/mallet', 'train-topics', '--input', 'data.mallet']\n",
    "print(terminal(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command opens data.mallet, and runs the topic modeling algorithm on it using the default settings, printing out the results as it goes. By default, mallet prints out the top 10 topics every 50th iteration. A good way to judge if the algorithm has converged is to look at the output. If it doesn't change much, it means that the algorithm converged.\n",
    "\n",
    "You can read more about the different options that can be used to fine tune the results [here.](http://mallet.cs.umass.edu/topics.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 <a id='exercise_2'></a>\n",
    "\n",
    "* Back to the [Table of Contents](#table)\n",
    "\n",
    "We ran the topic modeling algorithm, but we didn't save the output anywhere. If you look at the documentation pointed to above, it gives you different options to store the output. Modify the mallet command to output topic keys, topic composition of do <a id='exercise_1'></a>cuments, and a serialized MALLET topic trainer object. Add the option to enable hyperparameter optimization, increase the number of sampling iterations to 20,000, and increase the number of topics to 20. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "args = ['/bin/mallet/bin/mallet', 'train-topics', '--input', 'data.mallet', '--optimize-interval', '10', \\\n",
    "        '--output-topic-keys', 'topicKeys.txt', '--output-doc-topics', 'docTopics.txt', '--num-topics', '20', \\\n",
    "       '--num-iterations', '20000']\n",
    "print(terminal(args))\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at some results. Look at the topicKeys.txt and the docTopics.txt file. In topicKeys.txt, the first number is the topic (topic 0), and the second number gives an indication of the weight of that topic. \n",
    "\n",
    "docTopics.txt shows what topics compose your corpus of data. For example, abstract id 630248 had topic 0 as its main topic, at about 52%. Using this output, you can find connections between documents that you might not have realized otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing Topics <a id='inferencing_topics'></a>\n",
    "* Back to the [Table of Contents](#table)\n",
    "\n",
    "You can use your newly trained model to infer topics for unseen documents. Since we got the first 1000 abstracts to train the model, let us use the model to infer topics on the 1001th abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cursor.execute('SELECT * FROM TextAnalysis LIMIT 1 OFFSET 1000')\n",
    "data = cursor.fetchone()\n",
    "\n",
    "#Creating a new inference directory\n",
    "terminal(['mkdir', 'infer'])\n",
    "writeFile('./infer/' + str(data[\"APPLICATION_ID\"]) +\".txt\", data[\"ABSTRACT_TEXT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documentation for topic inference can be found [here](http://mallet.cs.umass.edu/topics.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 <a id='exercise_3'></a>\n",
    "* Back to the [Table of Contents](#table)\n",
    "\n",
    "Use the MALLET documentation to  <a id='exercise_1'></a>infer topic for the file we just downloaded in the infer folder. As mentioned in the documenation, make sure that the new data is compatible with your training data. Use the option --use-pipe-from [MALLET TRAINING FILE] in the MALLET command import-dir to specify a training file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will first need to rerun our model with the --inferencer-filename option\n",
    "args = ['/bin/mallet/bin/mallet', 'train-topics', '--input', 'data.mallet', '--optimize-interval', '10', \\\n",
    "        '--output-topic-keys', 'topicKeys.txt', '--output-doc-topics', 'docTopics.txt', '--num-topics', '20', \\\n",
    "       '--num-iterations', '20000', '--inferencer-filename', 'model.mallet']\n",
    "print(terminal(args))\n",
    "# Now import the one file that is in the infer folder and run the inferencer on it.\n",
    "### BEGIN SOLUTION\n",
    "args = ['/bin/mallet/bin/mallet', 'import-dir', '--input', './infer', '--output', 'one.mallet', '--use-pipe-from', \\\n",
    "        'data.mallet']\n",
    "print(terminal(args))\n",
    "\n",
    "args = ['/bin/mallet/bin/mallet', 'infer-topics', '--input', 'one.data', '--inferencer', 'model.mallet', \\\n",
    "  '--output-doc-topics', 'inf-one.txt', '--num-iterations', '10000']\n",
    "print(terminal(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources for Topic Modeling  <a id='resources'></a>\n",
    "\n",
    "* Back to the [Table of Contents](#table)\n",
    "\n",
    "Below you will find some tutorials and resources for topic modeling.\n",
    "- [General Introduction to Topic Modeling](https://www.cs.princeton.edu/~blei/papers/Blei2012.pdf)\n",
    "- [Topic Modeling for Humanists](http://www.scottbot.net/HIAL/?p=19113)\n",
    "- [Interpretation of Topic Models](http://www.umiacs.umd.edu/~jbg/docs/nips2009-rtl.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
