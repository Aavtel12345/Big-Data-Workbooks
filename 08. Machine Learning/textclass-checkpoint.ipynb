{
 "metadata": {
  "name": "",
  "signature": "sha256:d44c758d330473cea5ff060cd8afcc70854ac7173d90f7c11cfa0616286d3b5c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Document-level text analysis\n",
      "\n",
      "Document-level analysis is when you are interested in the whole text article, not tokens (sentences or words). The most basic example is labeling documents against some classification scheme, hence **text classification**. When you don't know your scheme ahead of time or you're interested in exploring a large set of data, you can try **topic modeling**.\n",
      "\n",
      "We're going to go over a couple of examples of document-level text analysis using some very most common classifiers models. We're going to go over the code to train your own model and discuss the results we see.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Supervised learning: Text classification in Python\n",
      "\n",
      "We're going to go over examples of how to use the excellent [Scikits-Learn](http://scikit-learn.org/stable/) library to train some text classifiers. \n",
      "\n",
      "The dataset used are the titles and topic codes from the `NYTimes` dataset that comes with the RTextTools library in `R`. It consists of titles from NYTimes front page news and associated codes according to [Amber Boydstun's classification scheme](http://www.policyagendas.org/sites/policyagendas.org/files/Boydstun_NYT_FrontPage_Codebook_0.pdf).\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn import metrics\n",
      "from operator import itemgetter\n",
      "from sklearn.metrics import classification_report\n",
      "import csv\n",
      "import os\n",
      "\n",
      "os.chdir('/home/rghani/census/textclassification/rjweiss-mozfest2013-a0ed5ba/data/')\n",
      "\n",
      "#note that if you generated this from R, you will need to delete the row\n",
      "#\"NYT_sample.Topic.Code\",\"NYT_sample.Title\"\n",
      "#from the top of the file.\n",
      "nyt = open('nyt_title_data.csv') # check the structure of this file!\n",
      "nyt_data = []\n",
      "nyt_labels = []\n",
      "csv_reader = csv.reader(nyt)\n",
      "\n",
      "for line in csv_reader:\n",
      " nyt_labels.append(int(line[0]))\n",
      " nyt_data.append(line[1])\n",
      "\n",
      "nyt.close()\n",
      "\n",
      "trainset_size = int(round(len(nyt_data)*0.75)) # i chose this threshold arbitrarily...to discuss\n",
      "print 'The training set size for this classifier is ' + str(trainset_size) + '\\n'\n",
      "\n",
      "X_train = np.array([''.join(el) for el in nyt_data[0:trainset_size]])\n",
      "y_train = np.array([el for el in nyt_labels[0:trainset_size]])\n",
      "\n",
      "X_test = np.array([''.join(el) for el in nyt_data[trainset_size+1:len(nyt_data)]]) \n",
      "y_test = np.array([el for el in nyt_labels[trainset_size+1:len(nyt_labels)]]) \n",
      "\n",
      "#print(X_train)\n",
      "\n",
      "vectorizer = TfidfVectorizer(min_df=2, \n",
      " ngram_range=(1, 2), \n",
      " stop_words='english', \n",
      " strip_accents='unicode', \n",
      " norm='l2')\n",
      " \n",
      "test_string = unicode(nyt_data[0])\n",
      "\n",
      "print \"Example string: \" + test_string\n",
      "print \"Preprocessed string: \" + vectorizer.build_preprocessor()(test_string)\n",
      "print \"Tokenized string:\" + str(vectorizer.build_tokenizer()(test_string))\n",
      "print \"N-gram data string:\" + str(vectorizer.build_analyzer()(test_string))\n",
      "print \"\\n\"\n",
      " \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The training set size for this classifier is 1621\n",
        "\n",
        "Example string: Dole Courts Democrats\n",
        "Preprocessed string: dole courts democrats\n",
        "Tokenized string:[u'Dole', u'Courts', u'Democrats']\n",
        "N-gram data string:[u'dole', u'courts', u'democrats', u'dole courts', u'courts democrats']\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.unique(nyt_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "array([ 3, 12, 15, 16, 19, 20, 29])"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = vectorizer.fit_transform(X_train)\n",
      "X_test = vectorizer.transform(X_test)\n",
      "\n",
      "nb_classifier = MultinomialNB().fit(X_train, y_train)\n",
      "\n",
      "y_nb_predicted = nb_classifier.predict(X_test)\n",
      "\n",
      "print \"MODEL: Multinomial Naive Bayes\\n\"\n",
      "\n",
      "print 'The precision for this classifier is ' + str(metrics.precision_score(y_test, y_nb_predicted))\n",
      "print 'The recall for this classifier is ' + str(metrics.recall_score(y_test, y_nb_predicted))\n",
      "print 'The f1 for this classifier is ' + str(metrics.f1_score(y_test, y_nb_predicted))\n",
      "print 'The accuracy for this classifier is ' + str(metrics.accuracy_score(y_test, y_nb_predicted))\n",
      "\n",
      "print '\\nHere is the classification report:'\n",
      "print classification_report(y_test, y_nb_predicted)\n",
      "\n",
      "#simple thing to do would be to up the n-grams to bigrams; try varying ngram_range from (1, 1) to (1, 2)\n",
      "#we could also modify the vectorizer to stem or lemmatize\n",
      "#q = np.unique(nyt_labels)\n",
      "#print '\\nHere is the confusion matrix:'\n",
      "#print metrics.confusion_matrix(y_test, y_nb_predicted, labels=q)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MODEL: Multinomial Naive Bayes\n",
        "\n",
        "The precision for this classifier is 0.678610380886\n",
        "The recall for this classifier is 0.549165120594\n",
        "The f1 for this classifier is 0.506785046956\n",
        "The accuracy for this classifier is 0.549165120594\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          3       1.00      0.23      0.38        47\n",
        "         12       0.75      0.08      0.15        37\n",
        "         15       1.00      0.10      0.19        39\n",
        "         16       0.59      0.56      0.58       112\n",
        "         19       0.46      0.88      0.60       162\n",
        "         20       0.70      0.64      0.67        99\n",
        "         29       1.00      0.21      0.35        43\n",
        "\n",
        "avg / total       0.68      0.55      0.51       539\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#What are the top N most predictive features per class?\n",
      "N = 10\n",
      "vocabulary = np.array([t for t, i in sorted(vectorizer.vocabulary_.iteritems(), key=itemgetter(1))])\n",
      "\n",
      "for i, label in enumerate(nyt_labels):\n",
      " if i == 7: # hack...\n",
      " break\n",
      " topN = np.argsort(nb_classifier.coef_[i])[-N:]\n",
      " print \"\\nThe top %d most informative features for topic code %s: \\n%s\" % (N, label, \" \".join(vocabulary[topN]))\n",
      " #print topN"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IndentationError",
       "evalue": "expected an indented block (<ipython-input-17-4cc2682868cc>, line 7)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-17-4cc2682868cc>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    break\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "svm_classifier = LinearSVC().fit(X_train, y_train)\n",
      "\n",
      "y_svm_predicted = svm_classifier.predict(X_test)\n",
      "print \"MODEL: Linear SVC\\n\"\n",
      "\n",
      "print 'The precision for this classifier is ' + str(metrics.precision_score(y_test, y_svm_predicted))\n",
      "print 'The recall for this classifier is ' + str(metrics.recall_score(y_test, y_svm_predicted))\n",
      "print 'The f1 for this classifier is ' + str(metrics.f1_score(y_test, y_svm_predicted))\n",
      "print 'The accuracy for this classifier is ' + str(metrics.accuracy_score(y_test, y_svm_predicted))\n",
      "\n",
      "print '\\nHere is the classification report:'\n",
      "print classification_report(y_test, y_svm_predicted)\n",
      "\n",
      "#simple thing to do would be to up the n-grams to bigrams; try varying ngram_range from (1, 1) to (1, 2)\n",
      "#we could also modify the vectorizer to stem or lemmatize\n",
      "print '\\nHere is the confusion matrix:'\n",
      "print metrics.confusion_matrix(y_test, y_svm_predicted, labels=unique(nyt_labels))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MODEL: Linear SVC\n",
        "\n",
        "The precision for this classifier is 0.63396261297\n",
        "The recall for this classifier is 0.623376623377\n",
        "The f1 for this classifier is 0.620561226142\n",
        "The accuracy for this classifier is 0.623376623377\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          3       0.69      0.47      0.56        47\n",
        "         12       0.43      0.49      0.46        37\n",
        "         15       0.68      0.44      0.53        39\n",
        "         16       0.60      0.59      0.59       112\n",
        "         19       0.60      0.77      0.67       162\n",
        "         20       0.72      0.66      0.69        99\n",
        "         29       0.73      0.56      0.63        43\n",
        "\n",
        "avg / total       0.63      0.62      0.62       539\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n"
       ]
      },
      {
       "ename": "NameError",
       "evalue": "name 'unique' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-18-72ccb273dcdc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#we could also modify the vectorizer to stem or lemmatize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'\\nHere is the confusion matrix:'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_svm_predicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnyt_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'unique' is not defined"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#What are the top N most predictive features per class?\n",
      "N = 10\n",
      "vocabulary = np.array([t for t, i in sorted(vectorizer.vocabulary_.iteritems(), key=itemgetter(1))])\n",
      "\n",
      "for i, label in enumerate(nyt_labels):\n",
      " if i == 7: # hack...\n",
      " break\n",
      " topN = np.argsort(svm_classifier.coef_[i])[-N:]\n",
      " print \"\\nThe top %d most informative features for topic code %s: \\n%s\" % (N, label, \" \".join(vocabulary[topN]))\n",
      " #print topN"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The top 10 most informative features for topic code 20: \n",
        "schiavo tissue baby scientists fat cancer gene medicare hospitals tobacco\n",
        "\n",
        "The top 10 most informative features for topic code 29: \n",
        "fallen limiting charged police rampage suspect murder crime sniper gun\n",
        "\n",
        "The top 10 most informative features for topic code 3: \n",
        "profit workers merger response storm pricing deal stocks enron microsoft\n",
        "\n",
        "The top 10 most informative features for topic code 16: \n",
        "base generals afghanistan navy force hussein nation nato 11 iraq\n",
        "\n",
        "The top 10 most informative features for topic code 19: \n",
        "pakistan india russian japan europe china africa mideast israel chinese\n",
        "\n",
        "The top 10 most informative features for topic code 19: \n",
        "impeachment whitewater race gingrich lewinsky senate president politics campaign democrats\n",
        "\n",
        "The top 10 most informative features for topic code 20: \n",
        "bowl armstrong match knicks playoffs play yankees series baseball game\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "maxent_classifier = LogisticRegression().fit(X_train, y_train)\n",
      "\n",
      "y_maxent_predicted = maxent_classifier.predict(X_test)\n",
      "print \"MODEL: Maximum Entropy\\n\"\n",
      "\n",
      "print 'The precision for this classifier is ' + str(metrics.precision_score(y_test, y_maxent_predicted))\n",
      "print 'The recall for this classifier is ' + str(metrics.recall_score(y_test, y_maxent_predicted))\n",
      "print 'The f1 for this classifier is ' + str(metrics.f1_score(y_test, y_maxent_predicted))\n",
      "print 'The accuracy for this classifier is ' + str(metrics.accuracy_score(y_test, y_maxent_predicted))\n",
      "\n",
      "print '\\nHere is the classification report:'\n",
      "print classification_report(y_test, y_maxent_predicted)\n",
      "\n",
      "#simple thing to do would be to up the n-grams to bigrams; try varying ngram_range from (1, 1) to (1, 2)\n",
      "#we could also modify the vectorizer to stem or lemmatize\n",
      "print '\\nHere is the confusion matrix:'\n",
      "print metrics.confusion_matrix(y_test, y_maxent_predicted, labels=unique(nyt_labels))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MODEL: Maximum Entropy\n",
        "\n",
        "The precision for this classifier is 0.654004346593\n",
        "The recall for this classifier is 0.549165120594\n",
        "The f1 for this classifier is 0.524774091511\n",
        "The accuracy for this classifier is 0.549165120594\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          3       0.94      0.32      0.48        47\n",
        "         12       0.67      0.16      0.26        37\n",
        "         15       0.71      0.13      0.22        39\n",
        "         16       0.64      0.54      0.59       112\n",
        "         19       0.44      0.85      0.58       162\n",
        "         20       0.72      0.60      0.65        99\n",
        "         29       1.00      0.28      0.44        43\n",
        "\n",
        "avg / total       0.65      0.55      0.52       539\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n"
       ]
      },
      {
       "ename": "NameError",
       "evalue": "name 'unique' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-19-e9b4a982ef0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#we could also modify the vectorizer to stem or lemmatize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'\\nHere is the confusion matrix:'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_maxent_predicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnyt_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'unique' is not defined"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#What are the top N most predictive features per class?\n",
      "N = 10\n",
      "vocabulary = np.array([t for t, i in sorted(vectorizer.vocabulary_.iteritems(), key=itemgetter(1))])\n",
      "\n",
      "for i, label in enumerate(nyt_labels):\n",
      " if i == 7: # hack...\n",
      " break\n",
      " topN = np.argsort(maxent_classifier.coef_[i])[-N:]\n",
      " print \"\\nThe top %d most informative features for topic code %s: \\n%s\" % (N, label, \" \".join(vocabulary[topN]))\n",
      " #print topN"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The top 10 most informative features for topic code 20: \n",
        "study scientists aids health hospitals medicare cancer care drug tobacco\n",
        "\n",
        "The top 10 most informative features for topic code 29: \n",
        "murder mexico officer drug gun suspect sniper police case crime\n",
        "\n",
        "The top 10 most informative features for topic code 3: \n",
        "big chief wall pay market billion stocks deal enron microsoft\n",
        "\n",
        "The top 10 most informative features for topic code 16: \n",
        "challenged iraqis nato force arms nation challenged war nation 11 iraq\n",
        "\n",
        "The top 10 most informative features for topic code 19: \n",
        "europe leader russia chinese japan russian mideast india israel china\n",
        "\n",
        "The top 10 most informative features for topic code 19: \n",
        "2000 dole clinton bush race senate politics president democrats campaign\n",
        "\n",
        "The top 10 most informative features for topic code 20: \n",
        "team play armstrong bowl knicks yankees playoffs series game baseball\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Unsupervised learning: Topic modeling in Python\n",
      "\n",
      "Now we're going to go over some typical topic modeling by using the popular [Gensim](http://radimrehurek.com/gensim/) library.\n",
      "\n",
      "The nice thing about Gensim is that it's ready to be applied to large datasets as it incorporates both the online version of LDA and distributed computing capability.\n",
      "\n",
      "We won't go over those features in this tutorial, since that would take hours to show a single example, and the NYTimes dataset is really quite small and can be run on a single machine."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim import corpora, models, similarities\n",
      "from itertools import chain\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from operator import itemgetter\n",
      "import re\n",
      "\n",
      "url_pattern = r'https?:\\/\\/(.*[\\r\\n]*)+'\n",
      "\n",
      "documents = [nltk.clean_html(document) for document in nyt_data]\n",
      "stoplist = stopwords.words('english')\n",
      "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
      " for document in documents]\n",
      "\n",
      "dictionary = corpora.Dictionary(texts)\n",
      "corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "\n",
      "tfidf = models.TfidfModel(corpus) \n",
      "corpus_tfidf = tfidf[corpus]\n",
      "\n",
      "#lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=100)\n",
      "#lsi.print_topics(20)\n",
      "\n",
      "n_topics = 60\n",
      "lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=n_topics)\n",
      "\n",
      "for i in range(0, n_topics):\n",
      " temp = lda.show_topic(i, 10)\n",
      " terms = []\n",
      " for term in temp:\n",
      "  terms.append(term[1])\n",
      "  print \"Top 10 terms for topic #\" + str(i) + \": \"+ \", \".join(terms)\n",
      " \n",
      "print \n",
      "print 'Which LDA topic maximally describes a document?\\n'\n",
      "print 'Original document: ' + documents[1]\n",
      "print 'Preprocessed document: ' + str(texts[1])\n",
      "print 'Matrix Market format: ' + str(corpus[1])\n",
      "print 'Topic probability mixture: ' + str(lda[corpus[1]])\n",
      "print 'Maximally probable topic: topic #' + str(max(lda[corpus[1]],key=itemgetter(1))[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named gensim",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-21-968df86edb19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mchain\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0moperator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mitemgetter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mImportError\u001b[0m: No module named gensim"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Big picture questions:\n",
      "\n",
      "1. How do the different supervised models compare against each other? \n",
      " 1. What's the tradeoffs between the metrics per model?\n",
      " 2. What about per class? Are some models better than others are certain classes?\n",
      " 3. What if we had had more data? Would some models get better than others?\n",
      " 5. What if our *observations* had more data? Instead of titles, we used lead paragraphs or even the full document?\n",
      " 6. What if our feature space was different? Instead of unigrams or bigrams, we used trigrams? Parts-of-speech?\n",
      " 4. Is there something about the **underlying language** structure that leads certain models to being better than others?\n",
      "2. How do the supervised models compare against the unsupervised model?\n",
      " 1. Are they \"better?\" If so, how?\n",
      " 2. What did we need to train a supervised model? What did we need to train an unsupervised model?\n",
      " 3. On that note, when is it more appropriate to use an unsupervised model over a supervised model?\n",
      " 4. How do you choose *k* number of topics for an unsupervised model?\n",
      " 5. What happens if you run the unsupervised model again? What about the supervised model?"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}